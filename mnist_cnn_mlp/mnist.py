# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15W97ZOyI6DJYBns39dv_5P4dWWkz_hZt
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

transform = transforms.Compose([
    transforms.ToTensor()
])

train_dataset = datasets.MNIST(root='.', train=True, transform=transform, download=True)
test_dataset  = datasets.MNIST(root='.', train=False, transform=transform, download=True)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

examples = enumerate(train_loader)
batch_idx, (example_data, example_targets) = next(examples)

plt.figure(figsize=(8, 4))
for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(example_data[i][0], cmap='gray')
    plt.title(f"Label: {example_targets[i].item()}")
plt.tight_layout()
plt.show()

class MNISTModel(nn.Module):
    def __init__(self):
        super().__init__()

        # Layer 1: 784 → 128
        self.fc1 = nn.Linear(28*28, 128)

        # Layer 2: 128 → 64
        self.fc2 = nn.Linear(128, 64)

        # Output Layer: 64 → 10 (digits 0–9)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        # Flatten from [64, 1, 28, 28] → [64, 784]
        x = x.view(-1, 28*28)

        # Layer 1 + ReLU
        x = F.relu(self.fc1(x))

        # Layer 2 + ReLU
        x = F.relu(self.fc2(x))

        # Output layer (NO softmax here, handled by loss)
        x = self.fc3(x)
        return x

model = MNISTModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

epochs = 5

for epoch in range(epochs):
    epoch_loss = 0
    for images, labels in train_loader:

        # ---- Forward Pass ----
        outputs = model(images)

        # ---- Loss Calculation ----
        loss = criterion(outputs, labels)

        # ---- Clear old gradients ----
        optimizer.zero_grad()

        # ---- Backpropagation ----
        loss.backward()

        # ---- Update weights ----
        optimizer.step()

        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}, Average Loss: {epoch_loss / len(train_loader):.4f}")

correct = 0
total = 0

with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")

images, labels = next(iter(test_loader))
outputs = model(images)
_, predicted = torch.max(outputs, 1)

plt.figure(figsize=(9, 4))
for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(images[i][0], cmap='gray')
    plt.title(f"True: {labels[i].item()}, Pred: {predicted[i].item()}")
plt.tight_layout()
plt.show()

