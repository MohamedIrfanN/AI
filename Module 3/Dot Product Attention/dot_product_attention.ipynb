{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "5fjIkiTT2cmT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q: Query matrix     (batch, seq_len, d_k)\n",
        "    K: Key matrix       (batch, seq_len, d_k)\n",
        "    V: Value matrix     (batch, seq_len, d_v)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. QKᵀ → similarity scores\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "    # 2. Scale by sqrt(d_k)\n",
        "    d_k = Q.size(-1)\n",
        "    scores = scores / math.sqrt(d_k)\n",
        "\n",
        "    # 3. Softmax → probabilities\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # 4. Weighted sum with V\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "bNYwnTk82oih"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GT5Hq9Pf2JCi"
      },
      "outputs": [],
      "source": [
        "batch = 1\n",
        "seq_len = 3\n",
        "d_k = 4\n",
        "d_v = 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = torch.randn(batch, seq_len, d_k)\n",
        "K = torch.randn(batch, seq_len, d_k)\n",
        "V = torch.randn(batch, seq_len, d_v)"
      ],
      "metadata": {
        "id": "Um3DTgy62Pl2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Attention Weights:\\n\", weights)\n",
        "print(\"\\nOutput:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCxCVbs52fwc",
        "outputId": "b09da054-48cb-47e5-baa5-ab82f307247b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " tensor([[[0.2166, 0.2821, 0.5013],\n",
            "         [0.0669, 0.1602, 0.7729],\n",
            "         [0.6856, 0.2844, 0.0300]]])\n",
            "\n",
            "Output:\n",
            " tensor([[[-0.5421, -0.8788,  0.0876,  0.5341],\n",
            "         [-0.8720, -1.3406,  0.3212,  1.1088],\n",
            "         [ 0.1682,  0.2629, -0.2504, -0.3174]]])\n"
          ]
        }
      ]
    }
  ]
}