# -*- coding: utf-8 -*-
"""dot_product_attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-QsowYB7op3viaaaicP8qzIk5gdjnJvi
"""

import torch
import torch.nn.functional as F
import math

def scaled_dot_product_attention(Q, K, V):
    """
    Q: Query matrix     (batch, seq_len, d_k)
    K: Key matrix       (batch, seq_len, d_k)
    V: Value matrix     (batch, seq_len, d_v)
    """

    # 1. QKᵀ → similarity scores
    scores = torch.matmul(Q, K.transpose(-2, -1))

    # 2. Scale by sqrt(d_k)
    d_k = Q.size(-1)
    scores = scores / math.sqrt(d_k)

    # 3. Softmax → probabilities
    attention_weights = F.softmax(scores, dim=-1)

    # 4. Weighted sum with V
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

batch = 1
seq_len = 3
d_k = 4
d_v = 4

Q = torch.randn(batch, seq_len, d_k)
K = torch.randn(batch, seq_len, d_k)
V = torch.randn(batch, seq_len, d_v)

output, weights = scaled_dot_product_attention(Q, K, V)

print("Attention Weights:\n", weights)
print("\nOutput:\n", output)