# -*- coding: utf-8 -*-
"""language_detection_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7xaQgsS-FWcufyQ-E4VOEknUSz9G6JE
"""

!pip install transformers datasets tokenizers accelerate

from datasets import load_dataset, Dataset
from transformers import (
    BertConfig, BertTokenizerFast, BertForSequenceClassification,
    TrainingArguments, Trainer
)
from tokenizers import ByteLevelBPETokenizer
import os

raw_ds = load_dataset("papluca/language-identification")
raw_ds

TARGET_LANGS = ["en", "fr", "es", "de", "hi", "ar"]

filtered_texts = []
filtered_labels = []

for item in raw_ds["train"]:
    if item["labels"] in TARGET_LANGS:
        filtered_texts.append(item["text"])
        filtered_labels.append(TARGET_LANGS.index(item["labels"]))

len(filtered_texts)

# -------------------------------------------------------------
# EXTRA: Add transliterated Hindi examples (Latin-script Hindi)
# -------------------------------------------------------------
extra_hindi_texts = [
    "Kutta sundar hai",
    "Mera naam Irfan hai",
    "Tum kaise ho",
    "Aaj mausam accha hai",
    "Bahut acha lagta hai",
    "Mujhe nahin pata",
    "Kya kar rahe ho",
    "Main theek hoon",
    "Yeh bahut important hai",
    "Mujhe Hindi bahut pasand hai",
    "Tumhara ghar kahan hai",
    "Kal milte hain",
    "Aaj khaane mein kya hai",
    "Mujhe pani chahiye",
    "Yeh kutta bada sundar hai",
]

for line in extra_hindi_texts:
    filtered_texts.append(line)
    filtered_labels.append(TARGET_LANGS.index("hi"))
# -------------------------------------------------------------
# EXTRA: Add transliterated Arabic examples (Arabizi / Franco-Arabic)
# -------------------------------------------------------------
extra_arabic_texts = [
    "el kalb gameel",
    "ana bahibbak",
    "enta fein",
    "aljaw helw elnaharda",
    "ana taaban geddan",
    "mish fahim",
    "mashy tamam",
    "al ayam di saaba shwaya",
    "inta betaamel eh",
    "mish aaref",
    "ana jaii delwa2ti",
    "mafeesh moshkela khalis",
    "kwayyes awi",
    "el makan da gamed",
    "el wad3 mashy tamam",
]

for line in extra_arabic_texts:
    filtered_texts.append(line)
    filtered_labels.append(TARGET_LANGS.index("ar"))

from datasets import ClassLabel, Features

# Convert labels to ClassLabel type
num_classes = len(TARGET_LANGS)
features = Features({'text': raw_ds['train'].features['text'], 'labels': ClassLabel(num_classes=num_classes, names=TARGET_LANGS)})

dataset = Dataset.from_dict({"text": filtered_texts, "labels": filtered_labels}, features=features)
dataset = dataset.train_test_split(test_size=0.2, stratify_by_column="labels")
dataset

from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained(
    "bert-base-multilingual-cased"
)

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=64
    )

tokenized_ds = dataset.map(tokenize_batch, batched=True)
tokenized_ds

config = BertConfig(
    vocab_size=tokenizer.vocab_size,
    hidden_size=128,
    num_hidden_layers=4,
    num_attention_heads=4,
    intermediate_size=256,
    max_position_embeddings=128,
    type_vocab_size=1,
    num_labels=len(TARGET_LANGS)
)

model = BertForSequenceClassification(config)
model

tokenized_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

training_args = TrainingArguments(
    output_dir="./bert_lang_detect",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    learning_rate=3e-4,
    weight_decay=0.01,
    logging_steps=50,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["test"],
    tokenizer=tokenizer
)

!pip uninstall -y wandb
os.environ["WANDB_DISABLED"] = "true"
trainer.train()

results = trainer.evaluate()
results

def predict_lang(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

    # NEW: move tokens to same device as model
    device = model.device
    tokens = {k: v.to(device) for k, v in tokens.items()}

    outputs = model(**tokens)
    pred = outputs.logits.argmax(dim=-1).item()
    return TARGET_LANGS[pred]


print(predict_lang("Der Hund ist schön"))                 # → de
print(predict_lang("dubee ek mahaan shahar hai"))         # → hi
print(predict_lang("الطقس جميل اليوم"))                   # → ar
print(predict_lang("Ismi Irfan"))                         # → ar
print(predict_lang("I love transformers"))                # → en
print(predict_lang("मेरा निवास दुबई में है"))                     # → hi