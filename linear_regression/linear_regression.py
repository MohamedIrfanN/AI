# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_lF3fcRDKCa2-lJNSMPhBphuChOaRnd
"""

import numpy as np
import matplotlib.pyplot as plt

# Study hours (x)
X = np.array([1, 2, 3, 4, 5, 6], dtype=float)

# Exam scores (y)
Y = np.array([52, 57, 65, 70, 75, 80], dtype=float)

global w, b
w = np.random.randn()  # weight (slope)
b = np.random.randn()  # bias (intercept)

def predict(x):
    return w * x + b

def compute_loss(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def compute_gradients(x, y_true, y_pred):
    dw = -2 * np.mean(x * (y_true - y_pred))
    db = -2 * np.mean(y_true - y_pred)
    return dw, db

learning_rate = 0.01
epochs = 1000

losses = []

for i in range(epochs):

    # 1. Make prediction
    y_pred = predict(X)

    # 2. Compute loss
    loss = compute_loss(Y, y_pred)
    losses.append(loss)

    # 3. Get gradients
    dw, db = compute_gradients(X, Y, y_pred)

    # 4. Update parameters
    w = w - learning_rate * dw
    b = b - learning_rate * db

    # Optional: print every 100 steps
    if i % 100 == 0:
        print(f"Epoch {i}, Loss: {loss:.4f}")

# Plot data points
plt.scatter(X, Y, color='blue', label='Actual Data')

line_x = np.linspace(min(X), max(X), 100)   # Smooth line for plotting
line_y = w * line_x + b                     # Model's learned line

plt.plot(line_x, line_y, color='red', label='Learned Line')

plt.xlabel("Hours Studied")
plt.ylabel("Score")
plt.title("Linear Regression From Scratch")
plt.legend()
plt.show()

print(f"Final Weight (w): {w}")
print(f"Final Bias (b): {b}")