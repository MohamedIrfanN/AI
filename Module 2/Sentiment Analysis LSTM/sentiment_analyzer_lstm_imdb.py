# -*- coding: utf-8 -*-
"""sentiment_analyzer_lstm_imdb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11qX3COSvIsiJ35YBNmrru_iEhP8nY3I0
"""

!pip install datasets tensorflow keras

import numpy as np
import tensorflow as tf
from datasets import load_dataset
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import re

# ---------------------------------------------------------
# 1. LOAD IMDB RAW TEXT
# ---------------------------------------------------------
dataset = load_dataset("imdb")

# Clean training text
texts = [
    re.sub(r"<br\s*/?>", " ", t.lower())  # remove HTML + lowercase
    for t in dataset["train"]["text"]
]
labels = np.array(dataset["train"]["label"])

# Clean test text
test_texts = [
    re.sub(r"<br\s*/?>", " ", t.lower())
    for t in dataset["test"]["text"]
]
test_labels = np.array(dataset["test"]["label"])

print("Dataset loaded.")
print("Training samples:", len(texts))
print("Testing samples:", len(test_texts))

# ---------------------------------------------------------
# 2. TOKENIZE
# ---------------------------------------------------------
vocab_size = 30000
max_length = 250

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

X = tokenizer.texts_to_sequences(texts)
X_test = tokenizer.texts_to_sequences(test_texts)

X = pad_sequences(X, maxlen=max_length, padding="post")
X_test = pad_sequences(X_test, maxlen=max_length, padding="post")

# ---------------------------------------------------------
# 3. SHUFFLE & SPLIT TRAIN/VAL (THIS FIXES THE ISSUE)
# ---------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(
    X, labels, test_size=0.2, random_state=42, shuffle=True
)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

# ---------------------------------------------------------
# 4. HIGH-ACCURACY STACKED LSTM MODEL
# ---------------------------------------------------------
embedding_dim = 128

model = Sequential([
    Input(shape=(max_length,)),
    Embedding(vocab_size, embedding_dim),

    # FIRST LSTM
    LSTM(128, dropout=0.3),

    Dense(64, activation='relu'),
    Dropout(0.3),

    Dense(1, activation='sigmoid')   # binary
])


model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model.summary()

# ---------------------------------------------------------
# 5. TRAIN WITH EARLY STOPPING
# ---------------------------------------------------------
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=64,
    callbacks=[early_stop]
)

# ---------------------------------------------------------
# 5. EVALUATE ON TEST DATASET
# ---------------------------------------------------------
loss, acc = model.evaluate(X_test, y_test)
print(f"\nüî• LSTM IMDB RAW TEXT Test Accuracy: {acc*100:.2f}%")

def clean_for_pred(text):
    text = text.lower()
    text = re.sub(r"<br\s*/?>", " ", text)
    text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def predict(text):
    text = clean_for_pred(text)
    seq = tokenizer.texts_to_sequences([text])
    pad = pad_sequences(seq, maxlen=max_length, padding="post")
    prob = model.predict(pad)[0][0]
    label = "Positive" if prob > 0.5 else "Negative"
    return label, prob

print("\nüß™ Custom Predictions:")
samples = [
    "I absolutely loved this movie! One of the best.",
    "This was terrible. I hated every moment.",
    "It was okay, not too bad, but not good either.",

    # üî• Strong Positive
    "What a fantastic film! I would watch it again.",
    "Amazing acting and a beautiful storyline.",
    "Loved every second of it, truly a masterpiece.",

    # ‚ùÑÔ∏è Strong Negative
    "One of the worst movies ever made. Total waste of time.",
    "Horrible acting, terrible script, zero entertainment value.",
    "I regret watching this, it was painfully bad.",

    # üôÇ Mild Positive
    "It was enjoyable and better than I expected.",
    "Not amazing, but I liked it overall.",
    "Pretty good movie, decent plot and good acting.",

    # üòê Neutral / Mixed
    "Some parts were great, others were boring.",
    "The movie was fine, nothing special though.",
    "It was an average film with a predictable storyline.",

    # üòí Mild Negative
    "The movie wasn‚Äôt terrible, but it was definitely disappointing.",
    "It dragged too much and felt really slow.",
    "Not the worst, but I wouldn‚Äôt recommend it.",

    # üåÄ Ambiguous / Hard to classify
    "I don‚Äôt know what to feel about this movie.",
    "Good visuals but weak story.",
    "The movie started strong but fell apart by the end.",

    # üòÇ Sarcastic (hard but good test)
    "Oh wow, what a brilliant movie‚Ä¶ said no one ever.",
    "Great‚Ä¶ another movie that put me to sleep."
]

for s in samples:
    label, p = predict(s)
    print(f"{s}\n‚Üí {label} ({p:.4f})\n")

