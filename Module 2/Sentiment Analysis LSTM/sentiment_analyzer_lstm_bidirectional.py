# -*- coding: utf-8 -*-
"""sentiment_analyzer_LSTM_bidirectional.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c4dEY49a-1ZJhn50enjUv6gi2Svq35R-
"""

!pip install datasets tensorflow keras

import numpy as np
import tensorflow as tf
from datasets import load_dataset
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import collections

# -------------------------------------------------------------
# 1. LOAD TWEETEVAL SENTIMENT DATASET (3 CLASSES)
#    Labels: 0 = Negative, 1 = Neutral, 2 = Positive
# -------------------------------------------------------------
dataset = load_dataset("tweet_eval", "sentiment")

train_texts = dataset["train"]["text"]
train_labels = dataset["train"]["label"]

val_texts   = dataset["validation"]["text"]
val_labels  = dataset["validation"]["label"]

test_texts  = dataset["test"]["text"]
test_labels = dataset["test"]["label"]

print("‚úÖ Dataset loaded!")
print("Train samples:", len(train_texts))
print("Val samples:  ", len(val_texts))
print("Test samples: ", len(test_texts))

# -------------------------------------------------------------
# 1A. INSPECT SOME DATA
# -------------------------------------------------------------
print("\nüîç SAMPLE TRAINING TWEETS (first 10):")
for i in range(10):
    print(f"Text : {train_texts[i]}")
    print(f"Label: {train_labels[i]}")  # 0=neg, 1=neutral, 2=pos
    print("-" * 60)

# Label distribution
label_counts = collections.Counter(train_labels)
print("\nüìä LABEL DISTRIBUTION (TRAIN SET):")
print("0 = Negative, 1 = Neutral, 2 = Positive\n")
for label, count in label_counts.items():
    print(f"Label {label}: {count} samples")

# -------------------------------------------------------------
# 2. TOKENIZE TEXT + PAD SEQUENCES
# -------------------------------------------------------------
vocab_size = 20000   # cap vocabulary size for speed & stability
max_length = 80      # max tokens per tweet (after padding/truncation)

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train_texts)

# Text ‚Üí integer sequences
X_train = tokenizer.texts_to_sequences(train_texts)
X_val   = tokenizer.texts_to_sequences(val_texts)
X_test  = tokenizer.texts_to_sequences(test_texts)

# Pad to fixed length for LSTM input
X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')
X_val   = pad_sequences(X_val,   maxlen=max_length, padding='post', truncating='post')
X_test  = pad_sequences(X_test,  maxlen=max_length, padding='post', truncating='post')

# One-hot encode labels (3 classes)
y_train = to_categorical(train_labels, num_classes=3)
y_val   = to_categorical(val_labels,   num_classes=3)
y_test  = to_categorical(test_labels,  num_classes=3)

print("\n‚úÖ Tokenization & padding complete.")
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

# -------------------------------------------------------------
# 3. BUILD Bidirectional LSTM MODEL WITH DROPOUT
# -------------------------------------------------------------
embedding_dim = 128

model = Sequential([
    Input(shape=(max_length,)),                     # (batch, 40)
    Embedding(vocab_size, embedding_dim),           # ‚Üí (batch, 40, 128)

    # Bidirectional LSTM: 64 units per direction ‚Üí 128-dim combined
    Bidirectional(
        LSTM(
            64,
            dropout=0.3,             # dropout on inputs
            recurrent_dropout=0.0    # keep 0.0 for speed; 0.3 is slower but possible
        )
    ),

    Dense(64, activation='relu'),
    Dropout(0.3),                                   # extra dropout on dense layer
    Dense(3, activation='softmax')                  # 3-class sentiment
])

model.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

print("\nüß† Model Summary:")
model.summary()

# -------------------------------------------------------------
# 4. EARLY STOPPING (ANTI-OVERFITTING)
# -------------------------------------------------------------
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)

# -------------------------------------------------------------
# 5. TRAIN THE MODEL (BiLSTM)
# -------------------------------------------------------------
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,              # early stopping will usually stop around 3‚Äì5
    batch_size=64,
    callbacks=[early_stop]
)

# -------------------------------------------------------------
# 6. EVALUATE ON TEST SET
# -------------------------------------------------------------
loss, acc = model.evaluate(X_test, y_test)
print(f"\nüî• BiLSTM Test Accuracy: {acc * 100:.2f}%")