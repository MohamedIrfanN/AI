# -*- coding: utf-8 -*-
"""sentiment_analyzer_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/128PkolAWksnEx-yWo3Y5IzzgIMhtVboJ
"""

!pip install datasets tensorflow keras

from datasets import load_dataset
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

# -------------------------------------------------------------
# 1. LOAD THE TWEETEVAL SENTIMENT DATASET
# Classes: 0 = Negative, 1 = Neutral, 2 = Positive
# -------------------------------------------------------------
dataset = load_dataset("tweet_eval", "sentiment")

train_texts = dataset["train"]["text"]
train_labels = dataset["train"]["label"]

val_texts = dataset["validation"]["text"]
val_labels = dataset["validation"]["label"]

test_texts = dataset["test"]["text"]
test_labels = dataset["test"]["label"]

print("Dataset loaded!")
print("Train samples:", len(train_texts))
print("Val samples:", len(val_texts))
print("Test samples:", len(test_texts))

print("\nüîç SAMPLE TRAINING DATA (first 10 examples):")
for i in range(10):
    print(f"Text:  {train_texts[i]}")
    print(f"Label: {train_labels[i]}")
    print("-" * 60)

# -------------------------------------------------------------
# 1B. SHOW LABEL DISTRIBUTION
# -------------------------------------------------------------
import collections
label_counts = collections.Counter(train_labels)

print("\nüìä LABEL DISTRIBUTION (TRAIN SET):")
print("0 = Negative")
print("1 = Neutral")
print("2 = Positive\n")
for label, count in label_counts.items():
    print(f"Label {label}: {count} samples")

# -------------------------------------------------------------
# 2. TOKENIZE THE TEXT
# -------------------------------------------------------------
vocab_size = 20000  # cap the vocab size for speed
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train_texts)

# Convert text ‚Üí sequences of integers
X_train = tokenizer.texts_to_sequences(train_texts)
X_val = tokenizer.texts_to_sequences(val_texts)
X_test = tokenizer.texts_to_sequences(test_texts)

# Pad to fixed length for LSTM
max_length = 40
X_train = pad_sequences(X_train, maxlen=max_length, padding='post')
X_val = pad_sequences(X_val, maxlen=max_length, padding='post')
X_test = pad_sequences(X_test, maxlen=max_length, padding='post')

# One-hot encode labels
y_train = to_categorical(train_labels, num_classes=3)
y_val = to_categorical(val_labels, num_classes=3)
y_test = to_categorical(test_labels, num_classes=3)

print("Tokenization & padding done!")

# -------------------------------------------------------------
# 3. BUILD LSTM MODEL
# -------------------------------------------------------------
embedding_dim = 128

model = Sequential([
    Input(shape=(max_length,)),
    Embedding(vocab_size, embedding_dim),
    LSTM(128, dropout=0.3),
    Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    Dense(3, activation='softmax') # 3 - Classes
])

model.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model.summary()

early_stop = EarlyStopping(
    monitor='val_loss',        # watch validation loss
    patience=2,                # stop after 2 bad epochs
    restore_best_weights=True  # go back to best model
)

# -------------------------------------------------------------
# 4. TRAIN THE MODEL
# -------------------------------------------------------------
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,            # keep low for speed; can increase to 5‚Äì8
    batch_size=64,
    callbacks = [early_stop]
)

# -------------------------------------------------------------
# 5. EVALUATE ON TEST SET
# -------------------------------------------------------------
loss, acc = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {acc*100:.2f}%")

# -------------------------------------------------------------
# 6. TEST ON CUSTOM SENTENCES
# -------------------------------------------------------------
def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_length, padding='post')
    pred = model.predict(padded)[0]

    classes = ["Negative", "Neutral", "Positive"]
    return classes[np.argmax(pred)], pred

samples = [
    "I absolutely love this!",
    "It's okay, nothing special.",
    "This is the worst thing ever.",
    "I am not sure how I feel about this.",
]

print("\nCustom Predictions:")
for s in samples:
    label, probs = predict_sentiment(s)
    print(f"Text: {s}")
    print("Prediction:", label)
    print("Probabilities:", probs)
    print()

