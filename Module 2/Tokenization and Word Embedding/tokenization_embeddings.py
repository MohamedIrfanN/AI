# -*- coding: utf-8 -*-
"""tokenization_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GcoznHkh9tuZ3lGCC_d_F35Kmeo5ilCg
"""

!pip install gensim

from gensim.utils import simple_preprocess
from gensim.models import Word2Vec

corpus = [
    "The quick brown fox jumps over the lazy dog",
    "The fox is quick and the dog is lazy",
    "I love learning about natural language processing",
    "Word embeddings capture semantic meaning of words",
    "The dog chased the fox yesterday",
    "Gensim makes training word embeddings easy"
]

tokenized_corpus = [simple_preprocess(doc) for doc in corpus]

print("Tokenized Corpus:")
for i, tokens in enumerate(tokenized_corpus):
    print(f"Sentence {i+1}: {tokens}")
print("\n")

model = Word2Vec(
    sentences=tokenized_corpus,
    vector_size=50,
    window=5,
    min_count=1,
    workers=4,
    sg=1
)

print("Word2Vec model trained successfully.\n")

print("Vocabulary Words:")
print(model.wv.index_to_key)

word = "fox"
print(f"\nEmbedding vector for '{word}':")
print(model.wv[word])
print(f"\nVector length: {len(model.wv[word])}")

print(f"\nMost similar words to '{word}':")
print(model.wv.most_similar(word))

from sklearn.cluster import KMeans
import numpy as np

vectors = np.array([model.wv[word] for word in model.wv.index_to_key])
words = model.wv.index_to_key

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(vectors)
labels = kmeans.labels_

for word, label in zip(words, labels):
    print(label, word)

import seaborn as sns
import numpy as np

words = model.wv.index_to_key[:10]  # first 10 words
matrix = np.zeros((10,10))

for i, w1 in enumerate(words):
    for j, w2 in enumerate(words):
        matrix[i,j] = model.wv.similarity(w1, w2)

sns.heatmap(matrix, xticklabels=words, yticklabels=words, annot=True, cmap="coolwarm")

