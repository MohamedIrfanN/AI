# -*- coding: utf-8 -*-
"""tokenization_embeddings_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VOFNcHeW23eATqIqLH_SJU6IDj7Z3a6n
"""

!pip install gensim

from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
from gensim.models.word2vec import Text8Corpus

!wget http://mattmahoney.net/dc/text8.zip
!unzip text8.zip

# Load the text8 dataset (already tokenized into sentences)

full_dataset = Text8Corpus('text8')

# Take first N sentences (each sentence = ~200â€“300 words)
dataset = []
for i, sentence in enumerate(full_dataset):
    if i == 200:   # first 200 sentences
        break
    dataset.append(sentence)

print("Loaded TEXT8 corpus successfully.\n")

# Convert Text8 generator into a list of tokenized sentences
dataset_list = list(dataset)

print("\nSample tokenized sentences from TEXT8:")
for i in range(5):     # print first 5 sentences
    print(f"Sentence {i+1}:", dataset_list[i])

model = Word2Vec(
    sentences=dataset,
    vector_size=200,     # Bigger vector since dataset is huge
    window=5,
    min_count=5,         # ignore rare words
    workers=4,
    sg=1                 # skip-gram
)

print("Word2Vec model trained successfully.\n")

print("Vocabulary size:", len(model.wv.index_to_key))
print("\nFirst 20 vocabulary words:")
print(model.wv.index_to_key[:20])

#Test Word Embeddings

word = "king"
if word in model.wv:
    print(f"\nEmbedding vector for '{word}':")
    print(model.wv[word])
    print(f"\nVector length: {len(model.wv[word])}")
else:
    print(f"\nWord '{word}' is not in the vocabulary.")

print(f"\nMost similar words to '{word}':")
print(model.wv.most_similar(word))

#K-Means Clustering
from sklearn.cluster import KMeans
import numpy as np

N = 50
subset_words = model.wv.index_to_key[:N]
subset_vectors = np.array([model.wv[w] for w in subset_words])

kmeans = KMeans(n_clusters=5, random_state=42)
labels = kmeans.fit_predict(subset_vectors)

print("\nK-Means clusters (first 50 words):")
for w, label in zip(subset_words, labels):
    print(label, w)

#Word similarity heat map
import seaborn as sns
import matplotlib.pyplot as plt

words = subset_words[:10]
matrix = np.zeros((10, 10))

for i, w1 in enumerate(words):
    for j, w2 in enumerate(words):
        matrix[i, j] = model.wv.similarity(w1, w2)

plt.figure(figsize=(8, 6))
sns.heatmap(matrix, xticklabels=words, yticklabels=words, annot=True, cmap="coolwarm")
plt.title("Word similarity heatmap (top 10 words)")
plt.show()